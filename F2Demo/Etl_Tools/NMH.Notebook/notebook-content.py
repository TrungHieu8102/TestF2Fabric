# Fabric notebook source

# METADATA ********************

# META {
# META   "kernel_info": {
# META     "name": "synapse_pyspark"
# META   },
# META   "dependencies": {
# META     "lakehouse": {
# META       "default_lakehouse": "41c8c31a-0f05-4bea-aad4-9b18da24a1b3",
# META       "default_lakehouse_name": "LKH_Silver_Data",
# META       "default_lakehouse_workspace_id": "8c76cb1c-cf62-4ebe-b9df-299509803689",
# META       "known_lakehouses": [
# META         {
# META           "id": "4b57cc09-e0a2-4ffd-80a0-4520f65d78f2"
# META         },
# META         {
# META           "id": "41c8c31a-0f05-4bea-aad4-9b18da24a1b3"
# META         }
# META       ]
# META     }
# META   }
# META }

# CELL ********************

import sys
from pyspark.sql.functions import col
from delta.tables import DeltaTable
from pyspark.sql.utils import AnalysisException
from datetime import datetime

# --- 1. C·∫•u h√¨nh ---
lakehouse_bronze_path = '8c76cb1c-cf62-4ebe-b9df-299509803689@onelake.dfs.fabric.microsoft.com/4b57cc09-e0a2-4ffd-80a0-4520f65d78f2'

# Danh s√°ch c√°c b·∫£ng c·∫ßn x·ª≠ l√Ω
dsbang = table_list.split(",")
# L·∫•y ng√†y h√¥m nay theo UTC
today_path = datetime.utcnow().strftime('%Y/%m/%d')

# --- 2. H√†m l·∫•y file parquet m·ªõi nh·∫•t ---
def get_latest_parquet_path(base_path):
    try:
        fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())
        files = fs.listStatus(spark._jvm.org.apache.hadoop.fs.Path(base_path))
        files_list = [file.getPath().toString() for file in files if file.getPath().toString().endswith(".parquet")]
        if not files_list:
            return None
        latest_file = max(
            files_list,
            key=lambda x: fs.getFileStatus(spark._jvm.org.apache.hadoop.fs.Path(x)).getModificationTime()
        )
        return latest_file
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói khi ƒë·ªçc th∆∞ m·ª•c {base_path}: {e}")
        return None

# --- 3. X·ª≠ l√Ω t·ª´ng b·∫£ng ---
for table_name in dsbang:
    print(f"\n=== X·ª≠ l√Ω b·∫£ng: {table_name} ===")

    base_path = f"abfss://{lakehouse_bronze_path}/Files/incremental_loads/{table_name}/{today_path}"
    latest_file = get_latest_parquet_path(base_path)

    if not latest_file:
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file parquet cho b·∫£ng {table_name}")
        continue

    print(f"üìÇ File m·ªõi nh·∫•t: {latest_file}")

    # ƒê·ªçc d·ªØ li·ªáu parquet
    df = spark.read.parquet(latest_file)

    # --- MERGE v√†o Silver ---
    try:
        if not spark.catalog.tableExists(table_name):
            print(f"‚ûï T·∫°o m·ªõi b·∫£ng Silver: {table_name}")
            df.write.format("delta").mode("overwrite").saveAsTable(table_name)
        else:
            print(f"üîÑ MERGE v√†o b·∫£ng Silver: {table_name}")
            delta_table_silver = DeltaTable.forName(spark, table_name)
            merge_condition = "silver.ID = bronze.ID"  # ƒë·ªïi theo PK th·ª±c t·∫ø

            delta_table_silver.alias("silver").merge(
                source=df.alias("bronze"),
                condition=merge_condition
            ).whenMatchedUpdateAll(
            ).whenNotMatchedInsertAll(
            ).execute()

        print(f"‚úÖ Ho√†n th√†nh x·ª≠ l√Ω b·∫£ng {table_name}")

    except Exception as e:
        print(f"‚ùå L·ªói khi x·ª≠ l√Ω b·∫£ng {table_name}: {e}")


# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark",
# META   "frozen": false,
# META   "editable": true
# META }

# CELL ********************

import sys
from pyspark.sql.functions import col
from delta.tables import DeltaTable
from pyspark.sql.utils import AnalysisException
from datetime import datetime, timedelta

# --- 1. C·∫•u h√¨nh c√°c bi·∫øn ---
# Thay 'your_lakehouse_name' b·∫±ng t√™n Lakehouse th·ª±c t·∫ø c·ªßa b·∫°n
lakehouse_bronze_path = '8c76cb1c-cf62-4ebe-b9df-299509803689@onelake.dfs.fabric.microsoft.com/4b57cc09-e0a2-4ffd-80a0-4520f65d78f2'

# --- 2. T√¨m file Parquet m·ªõi nh·∫•t ---
try:
    # L·∫•y ng√†y h√¥m qua
    today_path = datetime.utcnow().strftime('%Y/%m/%d')
    
    # X√¢y d·ª±ng ƒë∆∞·ªùng d·∫´n ƒë·∫ßy ƒë·ªß ƒë·∫øn th∆∞ m·ª•c v·ªõi t√™n Lakehouse
    base_path = f"abfss://{lakehouse_bronze_path}/Files/incremental_loads/{table_name}/{today_path}"
    
    # Li·ªát k√™ t·∫•t c·∫£ c√°c file trong th∆∞ m·ª•c
    files = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration()).listStatus(
        spark._jvm.org.apache.hadoop.fs.Path(base_path)
    )

    files_list = [file.getPath().toString() for file in files]
    
    if not files_list:
        print(f"Kh√¥ng t√¨m th·∫•y file n√†o trong th∆∞ m·ª•c: {base_path}")
        df = None
    else:
        # L·∫•y file m·ªõi nh·∫•t d·ª±a theo th·ªùi gian s·ª≠a ƒë·ªïi
        latest_file = max(files_list, key=lambda x: spark._jvm.org.apache.hadoop.fs.FileSystem.get(
            spark._jsc.hadoopConfiguration()).getFileStatus(spark._jvm.org.apache.hadoop.fs.Path(x)).getModificationTime())
        
        print(f"File m·ªõi nh·∫•t ƒë∆∞·ª£c t√¨m th·∫•y: {latest_file}")
        
        # ƒê·ªçc d·ªØ li·ªáu parquet t·ª´ file m·ªõi nh·∫•t
        df = spark.read.parquet(latest_file)

except Exception as e:
    print(f"L·ªói: {e}")
    df = None

# --- 3. Th·ª±c hi·ªán MERGE (Upsert) ---
if df:
    try:
        # Ki·ªÉm tra xem b·∫£ng ƒë√≠ch Silver ƒë√£ t·ªìn t·∫°i ch∆∞a b·∫±ng t√™n b·∫£ng
        if not spark.catalog.tableExists(table_name):
            print(f"B·∫£ng ƒë√≠ch {table_name} ch∆∞a t·ªìn t·∫°i. T·∫°o m·ªõi b·∫£ng t·ª´ d·ªØ li·ªáu Bronze.")
            # Ch·ªâ truy·ªÅn t√™n b·∫£ng v√†o saveAsTable
            df.write.format("delta").mode("overwrite").saveAsTable(table_name)
        else:
            print(f"B·∫£ng ƒë√≠ch {table_name} ƒë√£ t·ªìn t·∫°i. Th·ª±c hi·ªán MERGE.")

            # Ch·ªâ truy·ªÅn t√™n b·∫£ng v√†o forName
            delta_table_silver = DeltaTable.forName(spark, table_name)
            
            merge_condition = f"silver.ID = bronze.ID"
            
            delta_table_silver.alias("silver").merge(
                source = df.alias("bronze"),
                condition = merge_condition
            ).whenMatchedUpdateAll(
            ).whenNotMatchedInsertAll(
            ).execute()
            
        print(f"\nHo√†n th√†nh qu√° tr√¨nh MERGE cho b·∫£ng {table_name}.")

    except Exception as e:
        print(f"\nƒê√£ x·∫£y ra l·ªói trong qu√° tr√¨nh MERGE: {e}")
else:
    print("\nKh√¥ng c√≥ d·ªØ li·ªáu m·ªõi ƒë·ªÉ x·ª≠ l√Ω. Qu√° tr√¨nh upsert k·∫øt th√∫c.")

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark",
# META   "frozen": true,
# META   "editable": false
# META }
